{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOSsGxEMerthFx8rh21/PNq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCtt03dhiZom"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Patch_Embedding(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               in_channels:int=3,\n",
        "               out_channels:int=768,\n",
        "               stride:int=16,\n",
        "               patch_size:int=16):\n",
        "    super().__init__()\n",
        "    self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                             out_channels=out_channels,\n",
        "                             kernel_size=patch_size,\n",
        "                             stride=stride)\n",
        "    self.flatten = nn.Flatten(start_dim=2,\n",
        "                              end_dim=3)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x_patched = self.patcher(x.unsqueeze(0))\n",
        "    x_flattened = self.flatten(x_patched)\n",
        "\n",
        "\n",
        "    return x_flattened.permute(0,2,1)\n",
        "\n"
      ],
      "metadata": {
        "id": "9u1eEkk1l3Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedSelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               embedding_dim:int=768,\n",
        "               msa_heads:int=12,\n",
        "               attn_dropout:float=0.2\n",
        "               ):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    self.lyaer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    self.msa = nn.MultiheadAttention(\n",
        "                                    embed_dim=embedding_dim,\n",
        "                                    num_heads=msa_heads,\n",
        "                                    dropout=attn_dropout\n",
        "                                    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.lyaer_norm(x)\n",
        "    a,_ = self.msa(query=x,key=x,value=x)\n",
        "\n",
        "    return a"
      ],
      "metadata": {
        "id": "GIlolU62rmoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self,\n",
        "               embedding_dim:int=768,\n",
        "               mlp_heads:int=3072):\n",
        "\n",
        "    super().__init__()\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "    self.MlP = nn.Sequential(\n",
        "        nn.Dropout(p=0.2),\n",
        "        nn.Linear(in_features=embedding_dim,out_features=mlp_heads),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(in_features=mlp_heads,out_features=embedding_dim),\n",
        "        nn.Dropout(p=0.2)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.layer_norm(x)\n",
        "    x = self.MlP(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "5Goh3tGmwb_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               mlp_heads:int=3072,\n",
        "               msa_heads:int=12,\n",
        "               embedding_dim:int=768,\n",
        "               attn_dropout:float=0.2,\n",
        "               dropout:float=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.msa = MultiHeadedSelfAttention(embedding_dim,msa_heads,attn_dropout)\n",
        "    self.mlp = MLP(embedding_dim,mlp_heads)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.msa(x) + x\n",
        "    x = self.mlp(x) + x\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "bcQsrNIk5oKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size:int=224,\n",
        "               in_channels:int=3,\n",
        "               embedding_dim:int=768,\n",
        "               patch_size:int=16,\n",
        "               num_transformer_layers:int=12,\n",
        "               mlp_heads:int=3072,\n",
        "               msa_heads:int=12,\n",
        "               attn_dropout:float=0.2,\n",
        "               dropout:float=0.1):\n",
        "\n",
        "    super().__init__()\n",
        "    self.num_patches = (img_size * img_size) // patch_size**2\n",
        "    self.patch_embedding = Patch_Embedding(in_channels,\n",
        "                                   embedding_dim,\n",
        "                                   patch_size,\n",
        "                                   patch_size)\n",
        "    self.cls_token = nn.Parameter(torch.rand(1,1,embedding_dim),requires_grad=True)\n",
        "    self.pos_embedding = nn.Parameter(torch.rand(1,self.num_patches+1,embedding_dim),requires_grad=True)\n",
        "\n",
        "    self.encoder = nn.Sequential(*[TransformerEncoder(\n",
        "                                                      mlp_heads,\n",
        "                                                      msa_heads,\n",
        "                                                      embedding_dim,\n",
        "                                                      attn_dropout,\n",
        "                                                      dropout) for _ in range(num_transformer_layers)])\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "                                    nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "                                    nn.Linear(\n",
        "                                              in_features=embedding_dim,\n",
        "                                              out_features=1000)\n",
        "                                    )\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.patch_embedding(x)\n",
        "    batch_size = x.shape[0]\n",
        "    cls_token = self.cls_token.expand(batch_size,-1,-1)\n",
        "    x = torch.cat((cls_token,x),dim=1)\n",
        "\n",
        "    pos_embedding = self.pos_embedding\n",
        "    x = x + pos_embedding\n",
        "\n",
        "    x = self.encoder(x)\n",
        "\n",
        "    x = self.classifier(x[:,0])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XIVV1J1P4Ril"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = ViT(img_size=224,\n",
        "              in_channels=3,\n",
        "               embedding_dim=768,\n",
        "               patch_size=16,\n",
        "               num_transformer_layers=12,\n",
        "               mlp_heads=3072,\n",
        "               msa_heads=12,\n",
        "               attn_dropout=0.2,\n",
        "               dropout=0.1)"
      ],
      "metadata": {
        "id": "7iY9ZFp52CKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchinfo"
      ],
      "metadata": {
        "id": "RClx-1x0_Xxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model=a,row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOSs3Lf9_afy",
        "outputId": "67b198ba-9e3f-4be3-8772-d55426f18877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type (var_name))                                                Param #\n",
              "===============================================================================================\n",
              "ViT (ViT)                                                              152,064\n",
              "├─Patch_Embedding (patch_embedding)                                    --\n",
              "│    └─Conv2d (patcher)                                                590,592\n",
              "│    └─Flatten (flatten)                                               --\n",
              "├─Sequential (encoder)                                                 --\n",
              "│    └─TransformerEncoder (0)                                          --\n",
              "│    │    └─MultiHeadedSelfAttention (msa)                             2,363,904\n",
              "│    │    └─MLP (mlp)                                                  4,723,968\n",
              "│    └─TransformerEncoder (1)                                          --\n",
              "│    │    └─MultiHeadedSelfAttention (msa)                             2,363,904\n",
              "│    │    └─MLP (mlp)                                                  4,723,968\n",
              "│    └─TransformerEncoder (2)                                          --\n",
              "│    │    └─MultiHeadedSelfAttention (msa)                             2,363,904\n",
              "│    │    └─MLP (mlp)                                                  4,723,968\n",
              "│    └─TransformerEncoder (3)                                          --\n",
              "│    │    └─MultiHeadedSelfAttention (msa)                             2,363,904\n",
              "│    │    └─MLP (mlp)                                                  4,723,968\n",
              "│    └─TransformerEncoder (4)                                          --\n",
              "│    │    └─MultiHeadedSelfAttention (msa)                             2,363,904\n",
              "│    │    └─MLP (mlp)                                                  4,723,968\n",
              "│    └─TransformerEncoder (5)                                          --\n",
              "│    │    └─MultiHeadedSelfAttention (msa)                             2,363,904\n",
              "│    │    └─MLP (mlp)                                                  4,723,968\n",
              "│    └─TransformerEncoder (6)                                          --\n",
              "│    │    └─MultiHeadedSelfAttention (msa)                             2,363,904\n",
              "│    │    └─MLP (mlp)                                                  4,723,968\n",
              "│    └─TransformerEncoder (7)                                          --\n",
              "│    │    └─MultiHeadedSelfAttention (msa)                             2,363,904\n",
              "│    │    └─MLP (mlp)                                                  4,723,968\n",
              "│    └─TransformerEncoder (8)                                          --\n",
              "│    │    └─MultiHeadedSelfAttention (msa)                             2,363,904\n",
              "│    │    └─MLP (mlp)                                                  4,723,968\n",
              "│    └─TransformerEncoder (9)                                          --\n",
              "│    │    └─MultiHeadedSelfAttention (msa)                             2,363,904\n",
              "│    │    └─MLP (mlp)                                                  4,723,968\n",
              "│    └─TransformerEncoder (10)                                         --\n",
              "│    │    └─MultiHeadedSelfAttention (msa)                             2,363,904\n",
              "│    │    └─MLP (mlp)                                                  4,723,968\n",
              "│    └─TransformerEncoder (11)                                         --\n",
              "│    │    └─MultiHeadedSelfAttention (msa)                             2,363,904\n",
              "│    │    └─MLP (mlp)                                                  4,723,968\n",
              "├─Sequential (classifier)                                              --\n",
              "│    └─LayerNorm (0)                                                   1,536\n",
              "│    └─Linear (1)                                                      769,000\n",
              "===============================================================================================\n",
              "Total params: 86,567,656\n",
              "Trainable params: 86,567,656\n",
              "Non-trainable params: 0\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}